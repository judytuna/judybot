#!/usr/bin/env python3
"""
Test the LoRA-trained model by loading the base model + adapter.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os

def test_lora_model(
    base_model_name="microsoft/Phi-3.5-mini-instruct",
    adapter_path="./blog-model-unsloth-final"
):
    """Test the LoRA model by loading base + adapter."""
    print(f"ğŸ§ª Testing LoRA model")
    print(f"  Base model: {base_model_name}")
    print(f"  Adapter: {adapter_path}")

    if not os.path.exists(adapter_path):
        print(f"âŒ Adapter directory not found: {adapter_path}")
        return False

    try:
        print("\nğŸ“¦ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print("ğŸ§  Loading base model...")
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
        )

        print("ğŸ”§ Loading LoRA adapter...")
        model = PeftModel.from_pretrained(model, adapter_path)

        print("ğŸš€ Merging adapter for inference...")
        model = model.merge_and_unload()  # Merge LoRA weights for faster inference

        print("âœ… Model loaded successfully!")

        # Test prompts based on your training data themes
        test_prompts = [
            "What are your thoughts on programming?",
            "How do you approach creative projects?",
            "Tell me about a recent experience",
            "What's your opinion on technology?",
            "How do you stay motivated when working?",
            "What advice would you give about learning?",
        ]

        print(f"\nğŸ¯ Testing with {len(test_prompts)} prompts...")

        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n--- Test {i}/{len(test_prompts)} ---")
            print(f"ğŸ¤– Prompt: {prompt}")

            try:
                # Format prompt (using Phi-3.5 format)
                formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"

                # Tokenize
                inputs = tokenizer.encode(formatted_prompt, return_tensors="pt")
                if torch.cuda.is_available():
                    inputs = inputs.cuda()

                # Create attention mask
                attention_mask = torch.ones(inputs.shape, dtype=torch.long, device=inputs.device)

                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        attention_mask=attention_mask,
                        max_new_tokens=200,
                        temperature=0.7,
                        do_sample=True,
                        top_p=0.9,
                        repetition_penalty=1.1,
                        no_repeat_ngram_size=3,
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        early_stopping=True
                    )

                # Decode response
                full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

                # Extract just the new part
                if "<|assistant|>" in full_response:
                    response = full_response.split("<|assistant|>")[-1].strip()
                else:
                    response = full_response[len(formatted_prompt):].strip()

                # Clean up any remaining special tokens
                response = response.replace("<|end|>", "").strip()

                print(f"ğŸ“ Response: {response}")

                # Check for quality indicators
                if len(response) < 10:
                    print("âš ï¸  Short response - may need adjustment")
                elif len(response.split()) > 150:
                    print("âš ï¸  Very long response - consider shorter max_new_tokens")
                else:
                    print("âœ… Good response length")

            except Exception as e:
                print(f"âŒ Error generating response: {e}")
                continue

        print("\nğŸ‰ Testing complete!")

        # Memory cleanup
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return True

    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return False

def check_adapter_files(adapter_path="./blog-model-unsloth-final"):
    """Check adapter files."""
    print(f"ğŸ“ Checking adapter files in {adapter_path}...")

    if not os.path.exists(adapter_path):
        print(f"âŒ Adapter directory doesn't exist: {adapter_path}")
        return False

    files = os.listdir(adapter_path)
    print(f"ğŸ“„ Found {len(files)} files:")
    for file in sorted(files):
        file_path = os.path.join(adapter_path, file)
        size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        print(f"  {file} ({size:.1f} MB)")

    # Check for LoRA files
    has_adapter_config = "adapter_config.json" in files
    has_adapter_model = any(f.startswith("adapter_model") for f in files)
    has_tokenizer = any(f.startswith("tokenizer") for f in files)

    print(f"\nğŸ“‹ Adapter status:")
    print(f"  âœ… Adapter config: {has_adapter_config}")
    print(f"  âœ… Adapter weights: {has_adapter_model}")
    print(f"  âœ… Tokenizer: {has_tokenizer}")

    if has_adapter_config and has_adapter_model:
        print("âœ… LoRA adapter looks complete!")
        return True
    else:
        print("âŒ LoRA adapter incomplete")
        return False

def main():
    """Main test function."""
    print("ğŸ§ª LoRA Model Testing")
    print("=" * 25)

    # Check adapter files
    if not check_adapter_files():
        return False

    print("\n" + "=" * 40)

    # Test the model
    success = test_lora_model()

    if success:
        print("\nğŸ‰ Your blog style model is working!")
        print("ğŸ’¡ The model should now generate text in your writing style.")
    else:
        print("\nâŒ Model testing failed")
        print("ğŸ’¡ The training was successful, but there may be compatibility issues.")

    return success

if __name__ == "__main__":
    main()